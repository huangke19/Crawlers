## 概况 

本人2013年毕业于重庆大学，毕业后进入昆明供电局从事一次设备高压试验工作;2017年回到四川，2017年9⽉开始从事Python后端工作。 

## 个⼈信息 

黄科/男/1990年

重庆大学/电⽓工程与自动化 

Github: https://github.com/huangke19 

LeetCode: https://leetcode-cn.com/huangke19/ 

## 技能 

熟悉HTTP、TCP/IP协议和Charles, Wireshark等抓包工具

使用过Django，Flask，Flask-Restful，了解原生javascript

熟悉requests，Scrapy等库/框架的使用

熟悉MySQL和Redis，MongoDB的使⽤

熟悉Ubuntu, CentOS的使用，有现场实施经验

熟悉常用数据结构和算法，LeetCode刷题数122 [地址](https://github.com/huangke19/LeetCode) 



## Web经验

#### 2017/9 -- 2018/2 Python开发⼯工程师 

所属公司:成都仁和安信科技有限公司[5个月] 

项目描述:这是一套Python开发的基于Django框架的机顶盒娱乐系统，在系统上可以运行视频播放 业务和安卓游戏业务，该系统的客户端为机顶盒/电视，在前端通过原生JavaScript实现移动和选中等操作功能，在后台由Django进⾏逻辑处理。 

责任描述: 

1. 根据需求⽂档完成相应功能和接⼞的开发。 

   完成了运营内容分发接⼝的编写和使用，搭建FTP服务器，使⽤SOAP协议进行内容分发;负责了天津项⽬计费鉴权接⼞的设计和开发;完成了视频播放业务本地播放器的编写;配合进⾏了四川业务的数次版本迭代工作。 

2. 负责了天津联通TV的业务上线实施工作。 

   负责了服务器新机的Linux系统环境配置，使⽤Nginx和uwsgi布署业务，负责了机房的安装和网络配置，为了了适配天津标清机顶盒，将整体业务复制出一份进⾏改版;负责了和运营商⼈员的对接和接⼝调试，完成了五个省⼗余款机顶盒在测试环境的适配⼯作并成功通过审核上线⾄正式环境。 

3.负责问题排查、调试和⽂档的编写。 

#### 2018/4/16 -- 2018/6/16 Python开发工程师 

所属公司:北京三源合众公司 成都分公司 [2个月] 

项⽬:数据中心3D可视化管理系统，这是⼀个原有系统的重写，⽬的在于更换原系统的xweb框架为Flask框架，规范接口，同时将数据和前端展示分离，以使⼿机端和PC端功能一致。 

使⽤用技术: Flask, Flask-Restful, SQLAlchemy，MySQL 

责任描述: 

负责根据⽂档规划设计数据模型，并完成相应的后端RESTful接口编写，具体完成了机房管理模块和硬件设备管理模块的后端接口编写，并完成了⼀些临时的其他第三⽅项⽬的接⼝任务 

补充:六⽉公司资金出现问题，于是离职。离职后考取驾照，目前已拿到驾照，正在找工作。



## 个人爬虫经验

### 异步爬虫

- #### [电影天堂爬虫](https://github.com/huangke19/MovieSpider)

  使用异步库aiohttp进行爬取全站电影视频磁力链接，通过redis实现去重/增量爬取

### 动态网站爬虫

- 使用Chrome开发工具分析找到异步加载的信息并进行抓取

  #### [时光网票房爬虫](https://github.com/huangke19/MtimeSpider)

  #### [搜狗微信关键字爬虫](https://github.com/huangke19/WeixinSpider)

- 使用selenium自动化测试工具进行爬取

  #### [去哪儿网酒店信息爬虫](https://github.com/huangke19/QunarSpider)

### 手机APP爬虫

- #### [抖音爬虫](https://github.com/huangke19/TikTokSpider)

  通过抓包分析接口参数，实现输入用户id，下载用户所有视频作品的功能

- #### [酷我听书app爬虫](https://github.com/huangke19/KuwoSpider)

  使用wireshark和安卓模拟器配合分析找出酷我app的各级接口，从而抓取app内所有小说信息

### 整站爬虫

- #### [拉勾职位爬虫](https://github.com/huangke19/LagouSpider)

  使用requests和re模块进行爬取，使用mongodb进行存储

  使用Redis保存url队列实现分布式

  单机一天可爬取拉勾约300万url，爬取有效职位约17万个。

- #### [下厨房爬虫](https://github.com/huangke19/XiachufangSpider)

  使用Scrapy框架进行爬取

- #### [云起书院爬虫](https://github.com/huangke19/YunqiSpider)

  使用scrapy-redis搭建分布式爬虫

  使用mongodb搭建集群进行存储

#### [简单IP代理池](https://github.com/huangke19/IP_Pool)

- 通过Flask提供接口
- 通过requests和正则进行抓取
- 使用Redis的有序集合对IP进行保存
- 通过aiohttp库对代理进行异步检测
- 通过评分机制筛选淘汰不可用IP



#### 致谢 

感谢花时间阅读我的简历，期待能有机会和您共事。